---
title: "RIDER Experiment 1"
output:
  html_document:
    theme: sandstone
    toc: yes
    toc_depth: '1'
    df_print: paged

date: "2024-06-10"
author: Frieda Born
e-mail: born@mpib-berlin.mpg.de
---

This document provides an the statistical tests provided in the paper ordered along the sub sections of our publication. We investigate how testing and (de)prioritization during **Working** **Memory** influences how well something sticks to **Long-term** **Memory**. In the following you find the results for *RIDER1*. 
There are separate documents proving the statistical tests for Exp. 2 & Exp.. 3.
<br>
<br>
<br>

```{r setup environment, include=FALSE,fig.dim = c(4, 2)}
knitr::opts_chunk$set(echo = TRUE)
# surpress warnings
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
#clean environment
rm(list = ls()) 

#Libraries
library(plyr) #Ssplitting, applying and combining Data
library(dplyr) # for general data manipulation
library(ggplot2) # for visualizations
library(purrr)#working with functions and vectors
library(stringr)#working with strings
library(purrr)# for plotting
library(tidyverse)#different aspects of data processing
library(reshape)# for general data manipulation
library(scico)#colour palettes fpr plotting
library("readxl")
library(circular)
library(Rmisc)
library(ggpubr)# for plotting
library(rstatix)#for stats
library(viridis)# for nice plotting colors
library(data.table)
library(BayesFactor)#for performing a Bayesian t-test
library(cowplot)#for plotting
library(ggstatsplot)
library(formatR)
library(PupillometryR)
library(sm)
library(hrbrthemes)#nice plotting
library(viridis)#nice plotting
library(gridExtra)
library(plotly)# intersczive plotting
library(ggpubr)
`%notin%` <- Negate(`%in%`)
```

```{r loading data, include=FALSE}
# RIDER1 data
data_WM <- read.csv("/Users/born/Documents/Upside_down_task/client/public/raw_data/WM_data.csv",na.strings=c("","NA"))
data_LTM <- read.csv("/Users/born/Documents/Upside_down_task/client/public/raw_data/LTM_data.csv",na.strings=c("","NA"))
```


```{r check random assignment of probe orientataion, include=FALSE}
#Quick check that random probe orientation is not correlated with sample orientation.
#Separately for Sample 1 and Sample 2:
correlation_image1 <- cor(data_WM$startOri1, data_WM$start_distance_t1, method = "pearson")

# Printing the result
print(paste("For Sample 1, the correlation between WM sample and WM probe is:", correlation_image1))

data_WM_filtered <- na.omit(data_WM[, c("startOri2", "start_distance_t2")])

# Convert startOri2 and start_distance_t2 to numeric if they are not already
data_WM_filtered$startOri2 <- as.numeric(as.character(data_WM_filtered$startOri2))
data_WM_filtered$start_distance_t2 <- as.numeric(as.character(data_WM_filtered$start_distance_t2))

# Calculating correlation for Image 2 with filtered data
correlation_image2 <- cor(data_WM_filtered$startOri2, data_WM_filtered$start_distance_t2, method = "pearson")
# Printing the result
print(paste("Correlation Sample 2, the correlation between WM sample and WM probe is:", correlation_image2))
```
Please note: This markdown document entails the data processing steppwds 1) Trial selection, 
2) accuracy calculation (Error °), 
3) exp. condition definition, 
4) consistency data checks, 
5) pruning
6) Calculation of LTM bias by WM Report
These steps are not printed in the document, but run in the background when the script runs.


```{r calculating accurcy RIDER1 WM , include=FALSE}
### Processing of WM data (exp. trial selection, exp. condition identification, accuracy calculation)
# Group the data by participant and remove the first 6 trials for each participant (those where training trials)
data_WM <- data_WM %>%
  group_by(participant) %>%
  slice(7:n()) %>%
  ungroup()

#calculating the correct fixed orientations for the 2-item trials: depending on whether the stimulus was presented in image 1 or image 2
data_WM$startOri1 <- as.character(data_WM$startOri1)

data_WM <- data_WM %>%
  rowwise() %>%
  mutate(fixedOri_t1 = ifelse((test1 == image1), startOri1,ifelse((test1 ==  image2),startOri2,NA)))


data_WM <- data_WM %>%
  rowwise() %>%
  mutate(fixedOri_t2 = ifelse((test2 == image1), startOri1,ifelse((test2 ==  image2),startOri2,NA)))


# calculate remainder of this_ori variable to adjust for cases that are beyond 360 degrees
data_WM <- data_WM %>% mutate(ori_t1_corrected = this_ori_t1 %% 360)#Modulus (Remainder from division)
data_WM <- data_WM %>% mutate(ori_t2_corrected = this_ori_t2 %% 360)#Modulus (Remainder from division)

data_WM$fixedOri_t1 <- as.numeric(data_WM$fixedOri_t1)
data_WM$fixedOri_t2 <- as.numeric(data_WM$fixedOri_t2)



# function angular difference
angdiff <- function(alpha_deg, beta_deg) {
  #The function assumes that the input angles alpha and beta are in radians
  alpha_rad <- alpha_deg * pi / 180
  beta_rad <- beta_deg * pi / 180
  # calculation of the difference in angles
  delta_rad <- alpha_rad - beta_rad
  # If input angles alpha and beta are larger than 360°, the mod() function will correctly wrap them to the range of [-180,180] degrees interval.
  delta_rad <- (delta_rad + pi) %% (2*pi) - pi
  delta_deg <- delta_rad * 180 / pi
  #the function outputs both the angular difference in degrees and in radians
  return(list(delta_rad = delta_rad, delta_deg = delta_deg))
}

# accuracy in degrees
data_WM$accuracy_t1 <- abs(apply(data_WM[,c("this_ori_t1", "fixedOri_t1")], 1, function(x) angdiff(x[1], x[2])$delta_deg))
data_WM$accuracy_t2 <- abs(apply(data_WM[,c("this_ori_t2", "fixedOri_t2")], 1, function(x) angdiff(x[1], x[2])$delta_deg))



data_WM$accuracy_t1_s <-(apply(data_WM[,c("this_ori_t1", "fixedOri_t1")], 1, function(x) angdiff(x[1], x[2])$delta_deg))
data_WM$accuracy_t2_s <-(apply(data_WM[,c("this_ori_t2", "fixedOri_t2")], 1, function(x) angdiff(x[1], x[2])$delta_deg))

# pivot longer for easier data handling
data_WM_long<- data_WM %>% pivot_longer(cols=c('accuracy_t1','accuracy_t2'),names_to='attention_condition',values_to='accuracy') 

data_WM_long_s<- data_WM %>% pivot_longer(cols=c('accuracy_t1_s','accuracy_t2_s'),names_to='attention_condition',values_to='accuracy_s') 

# simple retrieval trial types
data_WM_long <- data_WM_long  %>% 
  mutate(trial_type_new = case_when(
    baselinePresent  == 1 ~ "Baseline",
    trial_type !="baseline" & attention_condition == "accuracy_t1"  ~ "Test 1",
    trial_type !="baseline" & attention_condition == "accuracy_t2" ~ "Test 2"))




data_WM_long_s <- data_WM_long_s  %>% 
  mutate(trial_type_new = case_when(
    baselinePresent  == 1 ~ "Baseline",
    trial_type !="baseline" & attention_condition == "accuracy_t1_s"  ~ "Test 1",
    trial_type !="baseline" & attention_condition == "accuracy_t2_s" ~ "Test 2"))




# image effect
data_WM_long <- data_WM_long  %>% 
  mutate(image_presentation = case_when(
    baselinePresent  == 1 ~ "Baseline",
    image1 == test1 & attention_condition == "accuracy_t1"  ~ "Img1-Test1",
    image1 == test2 & attention_condition == "accuracy_t2" ~ "Img1-Test2",
    image2 == test2 & attention_condition == "accuracy_t2" ~ "Img2-Test2",
    image2 == test1 & attention_condition == "accuracy_t1" ~ "Img2-Test1"))

data_WM_long <- data_WM_long %>%
  mutate(diff_Test1 = case_when(
    test2 != "_" & trial_type_new == "Test 1"  ~ "Yes",  # Condition for "Yes"
    test2 == "_" & trial_type_new == "Test 1"  ~ "No",  # Condition for "No"
  ))



# drop rows where we have no data
data_WM_long <-data_WM_long %>% drop_na(accuracy)

# one participant is excluded due to low performance in WM (see detailed RIDER1 script for wilcoxon test)
data_WM_long<-data_WM_long[!(data_WM_long$participant=='15'),]


# remove trials where reaction times are extremely long (> 15s)
# extract adjust_keys_t2_numeric from adjust_keys_t2.rt
data_WM_long$adjust_keys_t2_numeric <- apply(data_WM_long, 1, function(x) as.numeric(gsub("\\[|\\]", "", x["adjust_keys_t2.rt"])))
# extract adjust_keys_t1_numeric from adjust_keys_t1.rt
data_WM_long$adjust_keys_t1_numeric <- apply(data_WM_long, 1, function(x) as.numeric(gsub("\\[|\\]", "", x["adjust_keys_t1.rt"])))
data_WM$adjust_keys_t2_numeric <- apply(data_WM, 1, function(x) as.numeric(gsub("\\[|\\]", "", x["adjust_keys_t2.rt"])))
# extract adjust_keys_t1_numeric from adjust_keys_t1.rt
data_WM$adjust_keys_t1_numeric <- apply(data_WM, 1, function(x) as.numeric(gsub("\\[|\\]", "", x["adjust_keys_t1.rt"])))

# create a logical vector for filtering
filter_vec <- ((data_WM_long$adjust_keys_t2_numeric <= 15) | is.na(data_WM_long$adjust_keys_t2_numeric)) & ((data_WM_long$adjust_keys_t1_numeric <= 15) | is.na(data_WM_long$adjust_keys_t1_numeric))

# filter the data
data_WM_long <- data_WM_long[filter_vec,]


```



```{r calculating accurcy RIDER1 LTM , include=FALSE}
### Processing of LTM data (equal to the steps taken for WM results)

data_LTM<-data_LTM[!(data_LTM$participant=='15'),]

# preparing df for identification of trial types
data_LTM <- rbind.fill(data_LTM,data_WM[c("test1", "test2","participant","image1","image2")])



data_LTM <- data_LTM %>%
  group_by(participant)%>%
  mutate(trial_type = case_when(ltm_image %in% test1 & ltm_trial == 'baseline' ~ "Baseline",
                                 ltm_image %in%  test1  ~   "Test 1",
                                 ltm_image %in%  test2  ~   "Test 2",
                                 ltm_image %notin% test1 & ltm_image %notin% test2 ~ "Never tested"))


# Sample position effects
data_LTM <- data_LTM %>%
  group_by(participant)%>%
  mutate(image_presentation = case_when(ltm_trial == 'baseline' ~ "Baseline",
                                 ltm_image %in%  test1 & ltm_image %in% image1  ~   "Img1-Test1",
                                 ltm_image %in%  test1 & ltm_image %in% image2  ~   "Img2-Test1",
                                 ltm_image %in%  test2 & ltm_image %in% image1  ~   "Img1-Test2",
                                 ltm_image %in%  test2 & ltm_image %in% image2  ~   "Img2-Test2",
                                 ltm_image %notin% test1 & ltm_image %notin% test2 & ltm_image %in% image1 ~ "Img1-NT",
                                 ltm_image %notin% test1 & ltm_image %notin% test2 & ltm_image %in% image2 ~ "Img2-NT"))


##for checking for the influence of the other WM sample, we need to include information about the non-target as well


# calculate ltm accuracy
data_LTM <- data_LTM %>% mutate(ori_ltm = this_ori_ltm %% 360)#Modulus (Remainder from division)

data_LTM$accuracy_ltm <- abs(apply(data_LTM[,c("ori_ltm", "fixedOri_ltm")], 1, function(x) angdiff(x[1], x[2])$delta_deg))

data_LTM$accuracy_ltm_s <-(apply(data_LTM[,c("ori_ltm", "fixedOri_ltm")], 1, function(x) angdiff(x[1], x[2])$delta_deg))

#remove extra WM rows again
data_LTM <- data_LTM[rowSums(is.na(data_LTM)) == 0|rowSums(is.na(data_LTM)) == 1| rowSums(is.na(data_LTM)) == 2|rowSums(is.na(data_LTM)) == 3|rowSums(is.na(data_LTM)) == 4,]


wm_long <- data_WM %>%
  pivot_longer(
    cols = c(image1, image2),
    names_to = "image_position",
    values_to = "image"
  )

data_LTM_joined <- data_LTM %>%
  left_join(wm_long, by = c("participant", "ltm_image" = "image"))


```


```{r pruning LTM RIDER1, include=FALSE}

removed_trials <-data.frame(matrix(ncol=30,nrow=0, dimnames=list(NULL, colnames(data_WM_long))))
for (i in data_WM_long$participant){
  #select trials per participant
  t1_trials <- data_WM_long[which(data_WM_long$trial_type_new=='Test 1' & data_WM_long$participant == i), ]
  t2_trials <- data_WM_long[which(data_WM_long$trial_type_new=='Test 2' & data_WM_long$participant == i), ]
  baseline_trials <- data_WM_long[which(data_WM_long$trial_type_new=='Baseline' & data_WM_long$participant == i), ]
  #order mean values for each trial type to know what to prune
  vector <- c(mean(t1_trials$accuracy,na.rm = TRUE),mean(t2_trials$accuracy,na.rm = TRUE),mean(baseline_trials$accuracy,na.rm = TRUE))
  #find the reference for pruning (will be baseline for most participants)
  # Here I am chaninging min(vector) to mean(vector) to implement the same pruning as being performed in Exp. 3
  lowest <- mean(vector)+2 # +2 to avoid "over pruning"
  #order the single values to prepare to remove them to equalize the means
  t1_trials <- t1_trials[order(t1_trials$accuracy,decreasing = TRUE),]
  t2_trials <- t2_trials[order(t2_trials$accuracy,decreasing = TRUE),]
  baseline_trials <- baseline_trials[order(baseline_trials$accuracy,decreasing = TRUE),]
  while (mean(baseline_trials$accuracy,na.rm = TRUE) > lowest){
    removed <- baseline_trials[1,]
    baseline_trials <- baseline_trials[-1,]
    removed_trials[nrow(removed_trials) + 1, ] <- removed
  }
  while (mean(t1_trials$accuracy,na.rm = TRUE) > lowest){
    removed <- t1_trials[1,]
    t1_trials <- t1_trials[-1,]
    removed_trials[nrow(removed_trials) + 1, ] <- removed
  }
  while (mean(t2_trials$accuracy,na.rm = TRUE) > lowest){
    removed <- t2_trials[1,]
    t2_trials <- t2_trials[-1,]
    removed_trials[nrow(removed_trials) + 1, ] <- removed
  }
  print(i)
}

removed_trials <- removed_trials[!duplicated(removed_trials[,]),]


#write.csv(removed_trials,"/Users/born/Documents/Upside_down_task/client/public/removed_trials.csv", row.names = FALSE)
objects <- removed_trials %>%
  group_by(participant,trial_type_new)%>%
  summarise(test1,test2)

objects<- objects %>% pivot_longer(cols=c('test1', 'test2'),names_to='Presentation',values_to='image1') 

objects <- objects %>%
  rowwise()%>%
  mutate(delete = case_when(trial_type_new == "Test 1" & Presentation == "test2"  ~ "delete",
                            trial_type_new == "Test 2" & Presentation == "test1"  ~ "delete",
                            trial_type_new == "Test 1" & Presentation == "test1"  ~ "no_delete",
                            trial_type_new == "Test 2" & Presentation == "test2"  ~ "no_delete",
                            trial_type_new == "Baseline" & Presentation == "test2"  ~ "delete",
                            trial_type_new == "Baseline" & Presentation == "test1"  ~ "no_delete"))

objects<-objects[(objects$delete== "no_delete"),]

# identification of ltm object pool that includes only pruned WM conditions (Test 1 and Test 2 is effected here)
data_LTM <- rbind.fill(data_LTM,objects[c("participant", "image1")])

data_LTM <- data_LTM %>%
  group_by(participant)%>%
  mutate(matching = case_when(ltm_image %in% image1  ~ "Match",
                              ltm_image %notin%  image1  ~   "NO_Match"))


data_LTM <- data_LTM[rowSums(is.na(data_LTM)) == 1|rowSums(is.na(data_LTM)) == 2|rowSums(is.na(data_LTM)) == 3|rowSums(is.na(data_LTM)) == 4,]

# delete all matching trials in ltm dataframe
data_LTM_pruned<-data_LTM[!(data_LTM$matching=="Match"),]

data_LTM$pruned <- "NO"
data_LTM_pruned$pruned <-"YES"

## remove the trials with long rts > 15s 
data_LTM_pruned$adjust_keys_LTM.rt_numeric <- apply(data_LTM_pruned, 1, function(x) as.numeric(gsub("\\[|\\]", "", x["adjust_keys_LTM.rt"])))

# create a logical vector for filtering
filter_vec <- (data_LTM_pruned$adjust_keys_LTM.rt_numeric <= 15)

# filter the data
data_LTM_pruned <- data_LTM_pruned[filter_vec,]

```


```{r WM Bias RIDER1, include=FALSE}

data_LTM$adjust_keys_LTM.rt_numeric <- apply(data_LTM, 1, function(x) as.numeric(gsub("\\[|\\]", "", x["adjust_keys_LTM.rt"])))

# create a logical vector for filtering
filter_vec <- (data_LTM$adjust_keys_LTM.rt_numeric <= 15)

# filter the data
data_LTM <- data_LTM[filter_vec,]


test_2_data2 <- subset(data_LTM, trial_type == "Test 2", select = c("participant","ltm_image","this_ori_ltm","ori_ltm"))
test_2_data_WM2 <- subset(data_WM_long, trial_type_new == "Test 2", select = c("participant","test1","test2","ori_t1_corrected","ori_t2_corrected"))
test_2_data2 <- rbind.fill(test_2_data2,test_2_data_WM2[c("participant","test1","test2","ori_t1_corrected","ori_t2_corrected")])

test2_trials2 <-split(test_2_data2,test_2_data2$participant)

for(df in 1:length(test2_trials2)) {
  test2_trials2[[df]]$alternative_memory2 <- sapply(test2_trials2[[df]]$ltm_image,function(x) test2_trials2[[df]]$ori_t2_corrected[grep(x,test2_trials2[[df]]$test2)[1]])
}

test_2_data2 <- rbindlist(test2_trials2)

test_2_data2_a <- test_2_data2 %>% 
  rowwise() %>%
  mutate(min_angle_ltm = abs(ori_ltm - alternative_memory2))%>%
  mutate(max_angle_ltm = 360-abs(ori_ltm - alternative_memory2))

test_2_data2_s <- test_2_data2 %>%
  rowwise() %>%
  mutate(accuracy_ltm = ((ori_ltm - alternative_memory2 + 180) %% 360) - 180)

test_2_data2_a <- transform(test_2_data2_a, accuracy_ltm = pmin(min_angle_ltm, max_angle_ltm))#angular difference

# extract data for WM bias in baseline over participants

test_baseline_data2 <- subset(data_LTM, trial_type == "Baseline", select = c("participant","ltm_image","this_ori_ltm","ori_ltm"))
data_baseline2 <- subset(data_WM_long, trial_type_new == "Baseline", select = c("participant","test1","test2","ori_t1_corrected","ori_t2_corrected"))
test_baseline_data2 <- rbind.fill(test_baseline_data2,data_baseline2[c("participant","test1","test2","ori_t1_corrected","ori_t2_corrected")])

baseline_trials2 <-split(test_baseline_data2,test_baseline_data2$participant)

for(df in 1:length(baseline_trials2)) {
  baseline_trials2[[df]]$alternative_memory2 <- sapply(baseline_trials2[[df]]$ltm_image,function(x) baseline_trials2[[df]]$ori_t1_corrected[grep(x,baseline_trials2[[df]]$test1)[1]])
}

baseline_trials2 <- rbindlist(baseline_trials2)

baseline_trials2_a <- baseline_trials2 %>% 
  rowwise() %>%
  mutate(min_angle_ltm = abs(ori_ltm - alternative_memory2))%>%
  mutate(max_angle_ltm = 360-abs(ori_ltm - alternative_memory2))

baseline_trials2_s <- baseline_trials2 %>%
  rowwise() %>%
  mutate(accuracy_ltm = ((ori_ltm - alternative_memory2 + 180) %% 360) - 180)

baseline_trials2_a <- transform(baseline_trials2_a, accuracy_ltm = pmin(min_angle_ltm, max_angle_ltm))#angular difference

# extract data for WM bias in Test 1 over participants

test_1_data2 <- subset(data_LTM, trial_type == "Test 1", select = c("participant","ltm_image","this_ori_ltm","ori_ltm"))
test_1_data_WM2 <- subset(data_WM_long, trial_type_new == "Test 1", select = c("participant","test1","test2","ori_t1_corrected","ori_t2_corrected"))
test_1_data2 <- rbind.fill(test_1_data2,test_1_data_WM2[c("participant","test1","test2","ori_t1_corrected","ori_t2_corrected")])

test1_trials2 <-split(test_1_data2,test_1_data2$participant)

for(df in 1:length(test1_trials2)) {
  test1_trials2[[df]]$alternative_memory2 <- sapply(test1_trials2[[df]]$ltm_image,function(x) test1_trials2[[df]]$ori_t1_corrected[grep(x,test1_trials2[[df]]$test1)[1]])
}

test_1_data2 <- rbindlist(test1_trials2)


test_1_data2_a <- test_1_data2 %>% 
  rowwise() %>%
  mutate(min_angle_ltm = abs(ori_ltm - alternative_memory2))%>%
  mutate(max_angle_ltm = 360-abs(ori_ltm - alternative_memory2))

test_1_data2_a <- transform(test_1_data2_a, accuracy_ltm = pmin(min_angle_ltm, max_angle_ltm))#angular difference

test_1_data2_s <- test_1_data2 %>%
  rowwise() %>%
  mutate(
    accuracy_ltm = ((ori_ltm - alternative_memory2 + 180) %% 360) - 180
  )

# add trials type 
test_1_data2_a$trial_type <- "Test 1"
test_2_data2_a$trial_type <- "Test 2"
baseline_trials2_a$trial_type <- "Baseline"

test_1_data2_s$trial_type <- "Test 1"
test_2_data2_s$trial_type <- "Test 2"
baseline_trials2_s$trial_type <- "Baseline"


alternative_ltm2_a <- rbind(test_1_data2_a,test_2_data2_a,baseline_trials2_a)
alternative_ltm2_s <- rbind(test_1_data2_s,test_2_data2_s,baseline_trials2_s)

summary_stats_participant_WB_a <- summarySE(alternative_ltm2_a, measurevar = "accuracy_ltm", groupvars = c("trial_type","participant"),na.rm = T)
summary_study_level_WB_a <- summarySE(summary_stats_participant_WB_a, measurevar = "accuracy_ltm", groupvars = c("trial_type"),na.rm = T)

summary_stats_participant_WB_s <- summarySE(alternative_ltm2_s, measurevar = "accuracy_ltm", groupvars = c("trial_type","participant"),na.rm = T)
summary_study_level_WB_s <- summarySE(summary_stats_participant_WB_s, measurevar = "accuracy_ltm", groupvars = c("trial_type"),na.rm = T)


```

```{r intra trial bias data, include=FALSE}
# In response to a reviewer comment, I am checking for bias coming from another stimulus in a trial as well

data_WM <- data_WM %>%
  mutate(
    target_orientation_t1 = case_when(
      test1 == image1 ~ as.numeric(startOri1),
      test1 == image2 ~ as.numeric(startOri2),
      TRUE ~ NA_real_
    ),
    other_orientation_t1 = case_when(
      test1 == image1 ~ as.numeric(startOri2),
      test1 == image2 ~ as.numeric(startOri1),
      TRUE ~ NA_real_
    )
  )

data_WM <- data_WM %>%
  mutate(
    target_orientation_t2 = case_when(
      test2 == image1 ~ as.numeric(startOri1),
      test2 == image2 ~ as.numeric(startOri2),
      TRUE ~ NA_real_
    ),
    other_orientation_t2 = case_when(
      test2 == image1 ~ as.numeric(startOri2),
      test2 == image2 ~ as.numeric(startOri1),
      TRUE ~ NA_real_
    )
  )


data_WM <- data_WM %>%
  mutate(
    wm_error_t1 = angdiff(this_ori_t1, target_orientation_t1)$delta_deg,
    wm_error_t2 = angdiff(this_ori_t2, target_orientation_t2)$delta_deg
  )

data_WM <- data_WM %>%
  mutate(
    rel_angle_t1 = angdiff(other_orientation_t1, target_orientation_t1)$delta_deg,
    rel_angle_t2 = angdiff(other_orientation_t2, target_orientation_t2)$delta_deg
  )

# First, exclude baseline trials
df_filtered <- data_WM %>%
  filter(trial_type != "baseline")

# we  calculate the absolute angular difference to the non-target orientation

data_WM <- data_WM %>%
  mutate(
    error_to_other_t1 = abs(angdiff(this_ori_t1, other_orientation_t1)$delta_deg),
    error_to_other_t2 = abs(angdiff(this_ori_t2, other_orientation_t2)$delta_deg)
  )


# Filter for non-baseline trials
df_filtered <- data_WM %>%
  filter(trial_type != "baseline")

participant_means <- df_filtered %>%
  group_by(participant) %>%
  summarise(
    mean_error_to_other_t1 = mean(error_to_other_t1, na.rm = TRUE),
    mean_error_to_other_t2 = mean(error_to_other_t2, na.rm = TRUE)
  )

# Compute mean and SE across participants
summary_stats <- participant_means %>%
  summarise(
    mean_t1 = mean(mean_error_to_other_t1, na.rm = TRUE),
    se_t1   = sd(mean_error_to_other_t1, na.rm = TRUE) / sqrt(n()),
    mean_t2 = mean(mean_error_to_other_t2, na.rm = TRUE),
    se_t2   = sd(mean_error_to_other_t2, na.rm = TRUE) / sqrt(n())
  )

```

In the following, we are adding (signed) error distribution histograms across WM and LTM (sample/report).

```{r response error, echo=FALSE}
## In response to a reviewer comment, we inlcude a visualization of response error distributions here

# Prepare data
df <- data_WM_long_s %>%
  rename(`Trial Type` = trial_type_new) %>%
  mutate(`Trial Type` = recode(`Trial Type`, "Baseline" = "One-Sample"))

# Plot
breaks_seq <- seq(-180, 180, by = 2)
wm_hist <- ggplot(df, aes(x = accuracy_s, fill = `Trial Type`)) +
  geom_histogram(aes(y = ..density..), binwidth = 9, boundary = 0,
               position = "identity", alpha = 0.6, color = "black")+
  geom_density(alpha = 0.2) +
  geom_vline(data = df %>% group_by(`Trial Type`) %>% summarise(m = mean(accuracy_s, na.rm = TRUE)),
             aes(xintercept = m, color = `Trial Type`), linetype = "dashed", size = 1.2, show.legend = FALSE) +
  facet_wrap(~ `Trial Type`, nrow = 1, scales = "free_y")+
  scale_y_continuous(limits = c(0, 0.05))+
  scale_x_continuous(
    limits = c(-180, 180),
    breaks = seq(-180, 180, by = 90),
    expand = c(0, 0)
  )+
  scale_fill_manual(values = c("One-Sample" = "#a6dba0", "Test 1" = "#4c9085", "Test 2" = "#9e2e84")) +
  scale_color_manual(values = c("One-Sample" = "#a6dba0", "Test 1" = "#4c9085", "Test 2" = "#9e2e84")) +
  labs(
    title = "",
    x = "\nError (°)",
    y = "Density\n"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)
  )
# Show plot
print("In response to reviwer comments, in the following, we are showing (signed) error distributions in WM and LTM")
print("Histogram for WM errors")
print(wm_hist)


tiff("WM_hist2.tiff", units = "in", width = 14, height = 6, res = 300)
print(wm_hist)
dev.off()

df_ltm <- data_LTM %>%
  rename(`Trial Type` = trial_type) %>%
  mutate(`Trial Type` = recode(`Trial Type`,
                               "Baseline" = "One-Sample",
                               "Never tested" = "Not probed")) %>%
  filter(`Trial Type` != "Not probed") %>%
  mutate(`Trial Type` = factor(`Trial Type`, levels = c("One-Sample", "Test 1", "Test 2")))

ltm_hist <- ggplot(df_ltm, aes(x = accuracy_ltm_s, fill = `Trial Type`)) +
  geom_histogram(aes(y = ..density..), binwidth = 11, boundary = 0,
               position = "identity", alpha = 0.6, color = "black")+
  geom_density(alpha = 0.2) +
  geom_vline(data = df_ltm %>% group_by(`Trial Type`) %>% summarise(m = mean(accuracy_ltm_s, na.rm = TRUE)),
             aes(xintercept = m, color = `Trial Type`), linetype = "dashed", size = 1.2, show.legend = FALSE) +
  facet_wrap(~ `Trial Type`, nrow = 1, scales = "free_y") +
  scale_y_continuous(limits = c(0, 0.02)) +
  scale_x_continuous(
    limits = c(-180, 180),
    breaks = seq(-180, 180, by = 90),
    expand = c(0, 0)
  )+
  scale_fill_manual(values = c("One-Sample" = "#a6dba0", "Test 1" = "#4c9085", "Test 2" = "#9e2e84")) +
  scale_color_manual(values = c("One-Sample" = "#a6dba0", "Test 1" = "#4c9085", "Test 2" = "#9e2e84")) +
  labs(
    title = "",
    x = "\nError (°)",
    y = "Density\n"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)
  )

# Show plot
print("Histogram for LTM errors")
ltm_hist



# Optionally save it
#tiff("LTM_hist.tiff", units = "in", width = 14, height = 6, res = 300)
#print(ltm_hist)
#dev.off()



## Also calculating this for Rev 2 for the bias to reported orientation

df_ltm_alt <- alternative_ltm2_s %>%
  rename(`Trial Type` = trial_type) %>%
  mutate(`Trial Type` = recode(`Trial Type`,
                               "Baseline" = "One-Sample",
                               "Never tested" = "Not probed")) %>%
  filter(`Trial Type` != "Not probed") %>%
  mutate(`Trial Type` = factor(`Trial Type`, levels = c("One-Sample", "Test 1", "Test 2")))

ltm_hist_alt <- ggplot(df_ltm_alt, aes(x = accuracy_ltm, fill = `Trial Type`)) +
  geom_histogram(aes(y = ..density..), binwidth = 11, boundary = 0,
               position = "identity", alpha = 0.6, color = "black")+
  geom_density(alpha = 0.2) +
  geom_vline(data = df_ltm_alt %>% group_by(`Trial Type`) %>% summarise(m = mean(accuracy_ltm, na.rm = TRUE)),
             aes(xintercept = m, color = `Trial Type`), linetype = "dashed", size = 1.2, show.legend = FALSE) +
  facet_wrap(~ `Trial Type`, nrow = 1, scales = "free_y") +
  scale_y_continuous(limits = c(0, 0.02)) +
  scale_x_continuous(
    limits = c(-180, 180),
    breaks = seq(-180, 180, by = 90),
    expand = c(0, 0)
  )+
  scale_fill_manual(values = c("One-Sample" = "#a6dba0", "Test 1" = "#4c9085", "Test 2" = "#9e2e84")) +
  scale_color_manual(values = c("One-Sample" = "#a6dba0", "Test 1" = "#4c9085", "Test 2" = "#9e2e84")) +
  labs(
    title = "",
    x = "\nDeviation (°)",
    y = "Density\n"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    legend.position = "none",
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)
  )
# Save
#tiff("LTM_signed_error_hist.tiff", units = "in", width = 14.5, height = 6, res = 300)
print(ltm_hist_alt)
print("Histogram for LTM to WM report errors")
#dev.off()






```

# Statistics

<h3 style="text-align: center; color: orange;">**WM Performance**</h3>
#### pairwise comparisons and average accuracy across conditions
```{r WM Performance - > pairwise comparisons, echo=FALSE}
data_WM_long$task <-"WM"

# subsetting data
data_subset_WM <- data_WM_long%>%
  select(accuracy, trial_type_new,participant,task)

data_subset_WM_new <- data_WM_long%>%
  select(accuracy, trial_type_new,image_presentation,participant,task)

# renaming columns for consistency with LTM dfs
colnames(data_subset_WM)[2] = "exp_condition"
colnames(data_subset_WM_new)[2] = "exp_condition"
colnames(data_subset_WM_new)[3] = "stimulus_presentation"

# Set the order of levels in the exp_condition factor
data_subset_WM$exp_condition <- factor(data_subset_WM$exp_condition, levels = c("Test 2", "Test 1", "Baseline"))

data_summary_WM <-data_subset_WM %>%
  group_by(exp_condition, participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")

pairwise_comparisons_WM <- data_summary_WM %>%
  pairwise_t_test(
    mean ~ exp_condition, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes <- data_summary_WM %>%
  cohens_d(mean ~ exp_condition, paired = TRUE, var.equal = FALSE)

pairwise_comparisons_WM <- pairwise_comparisons_WM %>%
  left_join(effect_sizes, by = c(".y.", "group1", "group2"))

print("This table shows the 3 main pairwise comparisons across the WM exp. conditions")
print("Please also note that, as requested during revision, we have added the effect sizes (Cohen’s d) to our t-test results tables")

pairwise_comparisons_WM

# For averaging the two sample trial results, I am combining Test 1 and Test 2
data_subset_WM <- data_subset_WM %>%
  mutate(sample_trials = case_when(
    exp_condition == "Baseline" ~ 1,
    exp_condition %in% c("Test 1", "Test 2") ~ 2,
    TRUE ~ NA_real_  # Assign NA if none of the conditions match
  ))

# now I am again conducting the t-test

data_summary_WM_av <-data_subset_WM %>%
  group_by(sample_trials, participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")

pairwise_comparisons_WM_av <- data_summary_WM_av %>%
  pairwise_t_test(
    mean ~ sample_trials, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes_av <- data_summary_WM_av %>%
  cohens_d(mean ~ sample_trials, paired = TRUE, var.equal = FALSE)

pairwise_comparisons_WM_av <- pairwise_comparisons_WM_av %>%
  left_join(effect_sizes_av, by = c(".y.", "group1", "group2"))

print("This test shows difference between one sample (1) and two-sample (2) trials (averaged Test1/Test2")

pairwise_comparisons_WM_av

# Print the mean Error (°) across exp. conditions
summary_stats_participant_trial_type <- summarySE(data_WM_long, measurevar = "accuracy", groupvars = c("trial_type_new","participant"),na.rm = T)
mean_performnce_WM <- summarySE(data_WM_long, measurevar = "accuracy", groupvars = c("participant"),na.rm = T)

summary_study_level_trial_type <- summarySE(summary_stats_participant_trial_type, measurevar = "accuracy", groupvars = c("trial_type_new"),na.rm = T)

print("Average WM performance across exp. conditions:")

summary_study_level_trial_type

# Print the mean Error (°) across one-sample vs. two_sample

average_performance <- summarySE(data_subset_WM, measurevar = "accuracy", groupvars = c("sample_trials","participant"),na.rm = T)

average_performance <- summarySE(average_performance, measurevar = "accuracy", groupvars = c("sample_trials"),na.rm = T)

print("Average WM performance acorss one-sample (1) vs. two-sample (2) trials ")

average_performance


```

### ANOVA Results WM 
#### (2 x 2 ANOVA Test (1/2) and Sample (1/2)

```{r WM Performance - > ANOVAS, echo=FALSE}

print("This 2 x 2 ANOVA focuses on the priority and sample position effects in the WM task (1/2 sample, 1/2 test)")

filtered_data <- data_subset_WM_new %>%
  filter(stimulus_presentation %in% c("Img2-Test1", "Img1-Test1", "Img2-Test2", "Img1-Test2"))


filtered_data <- filtered_data %>%
  filter(exp_condition %in% c("Test 1", "Test 2"))

filtered_data <- filtered_data %>%
  mutate(stimulus_presentation = sub("-Test1$", "", stimulus_presentation),
         stimulus_presentation = sub("-Test2$", "", stimulus_presentation))


summary_image_pres <-filtered_data %>%
  group_by(exp_condition, participant,stimulus_presentation) %>%
  get_summary_stats(accuracy, type = "mean_sd")

# recode variable names to remove space in variable naming (otherwise error with anova_test function)
summary_image_pres <- summary_image_pres %>%
  mutate(exp_condition = recode(exp_condition,"Test 1" = "Test1","Test 2" = "Test2"))

#factor exp. condition and sample position
summary_image_pres$exp_condition <- factor(summary_image_pres$exp_condition)
summary_image_pres$stimulus_presentation <- factor(summary_image_pres$stimulus_presentation)

two_two_ANOVA_WM <- anova_test(data = summary_image_pres, dv = mean, wid = participant, within = c(stimulus_presentation,exp_condition))

get_anova_table(two_two_ANOVA_WM)

```
#### Post-hoc testing for 2 x 2 ANOVA

```{r WM Performance - > post-hoc 2x2 ANOVA, echo=FALSE}

post_hoc_sample_position <- summary_image_pres %>%
  group_by(exp_condition)%>%
  pairwise_t_test(
    mean ~ stimulus_presentation, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes <- summary_image_pres %>%
  group_by(exp_condition) %>%
  cohens_d(mean ~ stimulus_presentation, paired = TRUE, var.equal = FALSE)

post_hoc_sample_position <- post_hoc_sample_position %>%
  left_join(effect_sizes, by = c("exp_condition", ".y.", "group1", "group2"))

print("The following table shows the sample 1 /2 effects in WM Test 1 vs. Test 2; including effect size ")
post_hoc_sample_position


## Below we do the comparisons for the plot

# Sample 1 (Img1) — compare Test1 vs Test2
ttest_sample1 <- summary_image_pres %>%
  filter(stimulus_presentation == "Img1") %>%
  pairwise_t_test(
    mean ~ exp_condition, paired = TRUE,
    p.adjust.method = "none"
  ) %>%
  mutate(sample = "Sample 1")

# Sample 2 (Img2) — compare Test1 vs Test2
ttest_sample2 <- summary_image_pres %>%
  filter(stimulus_presentation == "Img2") %>%
  pairwise_t_test(
    mean ~ exp_condition, paired = TRUE,
    p.adjust.method = "none"
  ) %>%
  mutate(sample = "Sample 2")

effect_sample1 <- summary_image_pres %>%
  filter(stimulus_presentation == "Img1") %>%
  cohens_d(mean ~ exp_condition, paired = TRUE) %>%
  mutate(sample = "Sample 1")

effect_sample2 <- summary_image_pres %>%
  filter(stimulus_presentation == "Img2") %>%
  cohens_d(mean ~ exp_condition, paired = TRUE) %>%
  mutate(sample = "Sample 2")

print("The following comparisons, additionally, indicate sig. diff. when comparing Sample 1/2 in Test1/2")

ttest_sample1
effect_sample1
ttest_sample2
effect_sample2

summary_stats_participant_image_pres <- summarySE(data_WM_long, measurevar = "accuracy", groupvars = c("image_presentation","participant"),na.rm = T)
summary_study_level_image_pres <- summarySE(summary_stats_participant_image_pres, measurevar = "accuracy", groupvars = c("image_presentation"),na.rm = T)

print("Overview of Average WM performance across sample positions")

summary_study_level_image_pres
```

<h3 style="text-align: center; color: blue;">**LTM Performance**</h3>

#### pairwise comparisons and average accuracy across conditions

```{r LTM Performance - > pairwise comparisons, echo=FALSE}

summary_stats_participant_LTM <- summarySE(data_LTM, measurevar = "accuracy_ltm", groupvars = c("trial_type","participant"),na.rm = T)

mean_performance_LTM <- summarySE(data_LTM, measurevar = "accuracy_ltm", groupvars = c("participant"),na.rm = T)
summary_study_level_LTM <- summarySE(summary_stats_participant_LTM, measurevar = "accuracy_ltm", groupvars = c("trial_type"),na.rm = T)

print("The mean accuracy in the LTM is lower than in the WM task")
# Calculate the mean
mean_value <- mean(mean_performance_LTM$accuracy_ltm)
# Calculate the standard error
se_value <- sd(mean_performance_LTM$accuracy_ltm) / sqrt(length(mean_performance_LTM$accuracy_ltm))
# View the results
print(paste("Mean:", mean_value))
print(paste("Standard Error:", se_value))

t_test_result <- t.test(mean_performance_LTM$accuracy_ltm, mean_performnce_WM$accuracy,paired = TRUE)

t_test_result

# Calculate the difference between paired samples
differences <- mean_performance_LTM$accuracy_ltm - mean_performnce_WM$accuracy

# Compute the mean and standard deviation of the differences
mean_diff <- mean(differences)
sd_diff <- sd(differences)

# Compute Cohen's d for paired samples
cohen_d <- mean_diff / sd_diff

# Print result
cat("Cohen's d (paired):", round(cohen_d, 3), "\n")


print("Avergae mean performance across LTM conditions")
summary_study_level_LTM



data_LTM$task <- "LTM"

data_subset <- data_LTM %>%
  select(accuracy_ltm, trial_type,participant,task)

data_subset_new <- data_LTM %>%
  select(accuracy_ltm, trial_type,image_presentation,participant,task)

colnames(data_subset)[1] = "accuracy"
colnames(data_subset)[2] = "exp_condition"

colnames(data_subset_new)[1] = "accuracy"
colnames(data_subset_new)[2] = "exp_condition"
colnames(data_subset_new)[3] = "stimulus_presentation"

data_subset_filtered <- data_subset %>%
  filter(exp_condition %in% c("Baseline", "Test 1", "Test 2"))

data_summary_LTM <-data_subset_filtered %>%
  group_by(exp_condition, participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")

pairwise_comparisons_LTM<- data_summary_LTM %>%
  pairwise_t_test(
    mean ~ exp_condition, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes_LTM <- data_summary_LTM %>%
  cohens_d(mean ~ exp_condition, paired = TRUE, var.equal = FALSE)

# Join the effect sizes to the t-test results
pairwise_comparisons_LTM <- pairwise_comparisons_LTM %>%
  left_join(effect_sizes_LTM, by = c(".y.", "group1", "group2"))




print("This table shows the 3 main pairwise comparisons across the LTM exp. conditions")
pairwise_comparisons_LTM

# For averaging the two sample trial results, I am combining Test 1 and Test 2
data_subset <- data_subset %>%
  mutate(sample_trials = case_when(
    exp_condition == "Baseline" ~ 1,
    exp_condition %in% c("Test 1", "Test 2") ~ 2,
    exp_condition == "Never tested" ~ 3,
    TRUE ~ NA_real_  # Assign NA if none of the conditions match
  ))

# now I am again conducting the t-test

data_summary_LTM_av <-data_subset %>%
  group_by(sample_trials, participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")

pairwise_comparisons_LTM_av <- data_summary_LTM_av %>%
  pairwise_t_test(
    mean ~ sample_trials, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes_LTM_av <- data_summary_LTM_av %>%
  cohens_d(mean ~ sample_trials, paired = TRUE, var.equal = FALSE)

pairwise_comparisons_LTM_av <- pairwise_comparisons_LTM_av %>%
  left_join(effect_sizes_LTM_av, by = c(".y.", "group1", "group2"))


print("Here shown average difference between one-sample and two-sample in LTM")
pairwise_comparisons_LTM_av

summary_stats <- summarySE(data_summary_LTM_av, measurevar = "mean", groupvars = c("sample_trials"),na.rm = T)
summary_stats
```

### ANOVA Results LTM 
#### (2 x 2 ANOVA Test (1/2) and Sample (1/2)

```{r LTM Performance - > ANOVAS, echo=FALSE}

print("This 2 x 2 ANOVA focuses on the priority and sample position effects in the LTM task (1/2 sample, 1/2 test)")


data_subset_new<-data_subset_new[!(data_subset_new$exp_condition=="Baseline"),]

filtered_data_ltm <- data_subset_new %>%
  filter(stimulus_presentation %in% c("Img2-Test1", "Img1-Test1", "Img2-Test2", "Img1-Test2"))


filtered_data_ltm <- filtered_data_ltm %>%
  mutate(stimulus_presentation = sub("-Test1$", "", stimulus_presentation),
         stimulus_presentation = sub("-Test2$", "", stimulus_presentation))


summary_image_pres_ltm <-filtered_data_ltm %>%
  group_by(exp_condition, participant,stimulus_presentation) %>%
  get_summary_stats(accuracy, type = "mean_sd")

summary_image_pres_ltm <- summary_image_pres_ltm %>%
  mutate(exp_condition = recode(exp_condition,"Test 1" = "Test1","Test 2" = "Test2"))

# factor ANOVA levels
summary_image_pres_ltm$exp_condition <- factor(summary_image_pres_ltm$exp_condition)
summary_image_pres_ltm$stimulus_presentation<-factor(summary_image_pres_ltm$stimulus_presentation)

res.aov_image_pres_ltm <- anova_test(data = summary_image_pres_ltm, dv = mean, wid = participant, within = c(stimulus_presentation,exp_condition))

get_anova_table(res.aov_image_pres_ltm)

print("These post-hoc comparisons show the difference of sample position in Test 1 and Test 2(*)")

sample_pos_effects_ltm <- summary_image_pres_ltm %>%
  group_by(exp_condition)%>%
  pairwise_t_test(
    mean ~ stimulus_presentation, paired = TRUE,
    p.adjust.method = "holm")
  
effect_sizes_ltm <- summary_image_pres_ltm %>%
  group_by(exp_condition) %>%
  cohens_d(mean ~ stimulus_presentation, paired = TRUE, var.equal = FALSE)

# Join the effect sizes with the t-test results
sample_pos_effects_ltm <- sample_pos_effects_ltm %>%
  left_join(effect_sizes_ltm, by = c("exp_condition", ".y.", "group1", "group2"))
  
sample_pos_effects_ltm


```
#### Comparison tested and not probed (NP) items in LTM

```{r tested vs. NP, echo=FALSE}

data_summary_LTM_NP <-data_subset %>%
  group_by(exp_condition, participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")

pairwise_comparisons_LTM_NP<- data_summary_LTM_NP %>%
  pairwise_t_test(
    mean ~ exp_condition, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes_LTM_NP <- data_summary_LTM_NP %>%
  cohens_d(mean ~ exp_condition, paired = TRUE, var.equal = FALSE)

pairwise_comparisons_LTM_NP <- pairwise_comparisons_LTM_NP %>%
  left_join(effect_sizes_LTM_NP, by = c(".y.", "group1", "group2"))

print("This table shows the 3 main pairwise comparisons across the LTM exp. conditions vs. not probed")
pairwise_comparisons_LTM_NP

print("Here shown is the primacy effect also in the NP condition")

filtered_data_NP <- data_subset_new %>%
  filter(stimulus_presentation %in% c("Img1-NT", "Img2-NT"))

filtered_data_NP <- summarySE(filtered_data_NP, measurevar = "accuracy", groupvars = c("stimulus_presentation","participant"),na.rm = T)

sample_pos_effects_NP <- filtered_data_NP %>%
  pairwise_t_test(
    accuracy ~ stimulus_presentation, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes_NP <- filtered_data_NP %>%
  cohens_d(accuracy ~ stimulus_presentation, paired = TRUE, var.equal = FALSE)


sample_pos_effects_NP <- sample_pos_effects_NP %>%
  left_join(effect_sizes_NP, by = c(".y.", "group1", "group2"))

sample_pos_effects_NP




```
<h3 style="text-align: center; color: green;">**Comparison WM and LTM**</h3>

#### Comparing WM vs. LTM performance


#### ANOVA Results (2 x 2 x 2 ANOVA Test (1/2) and Sample (1/2) and task (WM/LTM)


```{r Task Comparsison - > WM / LTM, echo=FALSE}

full_data_new <- rbind(data_subset_WM_new, data_subset_new)

# preparation of data for desired 2x2x2 comparison
filtered_data2 <- full_data_new %>%
  filter(stimulus_presentation %in% c("Img2-Test1", "Img1-Test1", "Img2-Test2", "Img1-Test2"))


filtered_data2 <- filtered_data2 %>%
  mutate(stimulus_presentation = sub("-Test1$", "", stimulus_presentation),
         stimulus_presentation = sub("-Test2$", "", stimulus_presentation))

summary_image_pres2 <-filtered_data2 %>%
  group_by(exp_condition, participant,stimulus_presentation, task) %>%
  get_summary_stats(accuracy, type = "mean_sd")

summary_image_pres2 <- summary_image_pres2 %>%
  mutate(exp_condition = recode(exp_condition,"Test 1" = "Test1","Test 2" = "Test2"))

summary_image_pres2$exp_condition <- factor(summary_image_pres2$exp_condition)
summary_image_pres2$stimulus_presentation <- factor(summary_image_pres2$stimulus_presentation)
summary_image_pres2$task <- factor(summary_image_pres2$task)

res.aov_image_presWM2 <- anova_test(data = summary_image_pres2, dv = mean, wid = participant, within = c(stimulus_presentation,exp_condition,task))

get_anova_table(res.aov_image_presWM2)
```

<h3 style="text-align: center; color: purple;">**Pruning for equivalent WM performance**</h3>

#### pairwise comparisons and average performances across conditions



```{r Pruned data - > pairwise comparisons, echo=FALSE}
data_LTM_pruned$task <- "LTM"

data_subset_p <- data_LTM_pruned %>%
  select(accuracy_ltm, trial_type,participant,task)

data_subset_new_p <- data_LTM_pruned %>%
  select(accuracy_ltm, trial_type,image_presentation,participant,task)

LTM <- rbind(data_LTM,data_LTM_pruned)

colnames(data_subset_p)[1] = "accuracy"
colnames(data_subset_p)[2] = "exp_condition"
colnames(data_subset_new_p)[2] = "exp_condition"
colnames(data_subset_new_p)[1] = "accuracy"
colnames(data_subset_new_p)[3] = "stimulus_presentation"

data_subset_p<-data_subset_p[!(data_subset_p$exp_condition=="Never tested"),]

summary_ltm_p <-data_subset_p %>%
  group_by(exp_condition, participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")

# remove participants that through pruning lost complete dataset for single conditions
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="50"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="81"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="170"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="88"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="96"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="102"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="111"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="170"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="118"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="119"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="143"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="178"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="39"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="2"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="66"),]
#summary_ltm_p<-summary_ltm_p[!(summary_ltm_p$participant=="15"),]

trial_effects_p <- summary_ltm_p %>%
  pairwise_t_test(
    mean ~ exp_condition, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes_p <- summary_ltm_p %>%
  cohens_d(mean ~ exp_condition, paired = TRUE, var.equal = FALSE)

trial_effects_p <- trial_effects_p %>%
  left_join(effect_sizes_p, by = c(".y.", "group1", "group2"))

print("Pairwise comparisons of pruned data exp. conditions")
trial_effects_p

summary_stats_LTM_P <- summarySE(summary_ltm_p, measurevar = "mean", groupvars = c("exp_condition"),na.rm = T)

print("Avergae performances in pruned data")
summary_stats_LTM_P

```
<h3 style="text-align: center; color: pink;">**LTM recall of WM sample vs. WM report**</h3>

#### Pairwise comparisons of WM sample vs report and 2 x 3 repeated measures ANOVA

```{r WM Reports pairwise comparisons, echo=FALSE}
WM_bias <-alternative_ltm2_a %>%
  group_by(trial_type, participant) %>%
  get_summary_stats(accuracy_ltm, type = "mean_sd")

print("Here shown are the pairwise comparisons for LTM accuracy of WM Report (instead of sample)")

WM_bias %>%
  pairwise_t_test(
    mean ~ trial_type, paired = TRUE,
    p.adjust.method = "holm")

colnames(WM_bias)[2] = "exp_condition"

WM_bias2 <-alternative_ltm2_a %>%
  group_by(participant) %>%
  get_summary_stats(accuracy_ltm, type = "mean_sd")

# Filtering to remobe NP condition, as this one cannot have data of WM reports
data_subset_filtered <- data_subset %>%
  filter(exp_condition %in% c("Baseline", "Test 1", "Test 2"))

data_summary_LTM <-data_subset_filtered %>%
  group_by(participant) %>%
  get_summary_stats(accuracy, type = "mean_sd")


# Comparison of LTM accuracy of WM Sample vs. Report

t_test_result <- t.test(data_summary_LTM$mean, WM_bias2$mean,paired = TRUE)

print("Here shown is the pairwise comparison of LTM accuracy Sample vs. Report")
mean_diff <- mean(data_summary_LTM$mean - WM_bias2$mean, na.rm = TRUE)
sd_diff <- sd(data_summary_LTM$mean - WM_bias2$mean, na.rm = TRUE)
cohen_d <- mean_diff / sd_diff


cat(sprintf("t(%d) = %.2f, p = %.3f, Cohen's d = %.3f\n",
            t_test_result$parameter,
            t_test_result$statistic,
            t_test_result$p.value,
            cohen_d))

data_summary_LTM2 <-data_subset_filtered %>%
  group_by(participant,exp_condition ) %>%
  get_summary_stats(accuracy, type = "mean_sd")

WM_bias$type <- "Report"
data_summary_LTM2$type <- "Sample"

sample_vs_report <- rbind(WM_bias,data_summary_LTM2)


sample_vs_report_comparison <- sample_vs_report %>%
  group_by(exp_condition)%>%
  pairwise_t_test(
    mean ~ type, paired = TRUE,
    p.adjust.method = "holm")

effect_sizes <- sample_vs_report %>%
  group_by(exp_condition) %>%
  cohens_d(mean ~ type, paired = TRUE, var.equal = FALSE)


sample_vs_report_comparison <- sample_vs_report_comparison %>%
  left_join(effect_sizes, by = c("exp_condition", ".y.", "group1", "group2"))

sample_vs_report_comparison

## One way ANOVA to proof that bias is sig. factor

sample_vs_report <- sample_vs_report %>%
  mutate(exp_condition = recode(exp_condition,"Test 1" = "Test1","Test 2" = "Test2"))

sample_vs_report$exp_condition <- factor(sample_vs_report$exp_condition)

sample_vs_report$type <- factor(sample_vs_report$type)

res.aov_bias <- anova_test(data = sample_vs_report, dv = mean, wid = participant, within = c(exp_condition,type))
get_anova_table(res.aov_bias)
```


```{r WM Reports, echo=FALSE}
summary_study_level_LTM$WM_point <- "WM_sample"
summary_stats_participant_LTM$WM_point <- "WM_sample"
summary_study_level_WB_a$WM_point <- "WM_report"
summary_stats_participant_WB_a$WM_point <- "WM_report"

summary_stats_participant_LTM$participant <- factor(summary_stats_participant_LTM$participant)
summary_stats_participant_WB_a$participant <- factor(summary_stats_participant_WB_a$participant)

combined_participant_data <- bind_rows(summary_stats_participant_LTM,summary_stats_participant_WB_a)

combined_participant_data$trial_type[combined_participant_data$trial_type == 'Baseline'] <- "One-Sample"
combined_participant_data$trial_type[combined_participant_data$trial_type == 'One-item prioritized'] <- "One-Sample"
combined_participant_data$trial_type[combined_participant_data$trial_type == 'Two-item prioritized'] <- "Test 1"
combined_participant_data$trial_type[combined_participant_data$trial_type == 'Two-item deprioritized'] <- "Test 2"
combined_participant_data$trial_type[combined_participant_data$trial_type == 'Never tested'] <- "Not Probed"

# Filter for specified trial types
filtered_df <- combined_participant_data %>%
  filter(trial_type %in% c("One-Sample", "Test 1", "Test 2"))

# Calculate mean and standard error by WM_point
summary_df <- filtered_df %>%
  group_by(WM_point) %>%
  summarize(
    mean_accuracy_ltm = mean(accuracy_ltm, na.rm = TRUE),
    se_accuracy_ltm = sd(accuracy_ltm, na.rm = TRUE) / sqrt(n())
  )

# Print the result
print(summary_df)

# Now I want to calculate a t-test in the WM_report vs. WM_sample (of baseline, test 1, test 2 performance per participant)

# Pivot the data to wide format
wide_df <- filtered_df %>%
  pivot_wider(names_from = WM_point, values_from = accuracy_ltm) %>%
  dplyr::rename(WM_sample_accuracy = WM_sample, WM_report_accuracy = WM_report)


summary_df2 <- wide_df %>%
  group_by(participant) %>%
  summarize(
    WM_sample_accuracy = mean(WM_sample_accuracy, na.rm = TRUE),
    WM_report_accuracy = mean(WM_report_accuracy, na.rm = TRUE)
  )

# Perform the paired t-test
t_test_result <- t.test(summary_df2$WM_sample_accuracy, summary_df2$WM_report_accuracy, paired = TRUE)

# Print the t-test results
print(t_test_result)

## I want to check if the WM report bias increaes over conditions:
# Convert the participant column from factor to integer in the first data frame
summary_stats_participant_LTM$participant <- as.integer(as.character(summary_stats_participant_LTM$participant))

summary_stats_participant_LTM$participant <- factor(summary_stats_participant_LTM$participant)
summary_stats_participant_WB_a$participant <- factor(summary_stats_participant_WB_a$participant)

combined_sample_report <- bind_rows(summary_stats_participant_LTM, summary_stats_participant_WB_a)

combined_sample_report <- combined_sample_report %>%
  filter(trial_type != "Never tested")

combined_sample_report <- combined_sample_report %>%
  mutate(trial_type = case_when(
    trial_type == "One-item prioritized" ~ "Baseline",
    trial_type == "Two-item prioritized" ~ "Test 1",
    trial_type == "Two-item deprioritized" ~ "Test 2",
    TRUE ~ trial_type
  ))

combined_sample_report_wide <- combined_sample_report %>%
  pivot_wider(
    names_from = WM_point, 
    values_from = accuracy_ltm,
    names_prefix = "Accuracy_"
  ) %>%
  # Now you have Accuracy_WM_report and Accuracy_WM_sample for each row
  
  group_by(participant, trial_type) %>%
  summarize(
    Accuracy_WM_report = mean(Accuracy_WM_report, na.rm = TRUE),
    Accuracy_WM_sample = mean(Accuracy_WM_sample, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  # Calculate WM bias
  mutate(WM_bias = Accuracy_WM_report - Accuracy_WM_sample)

combined_sample_report_wide <- combined_sample_report_wide %>%
  mutate(Condition_Numeric = case_when(
    trial_type == "Baseline" ~ 0,
    trial_type == "Test 1" ~ 1,
    trial_type == "Test 2" ~ 2,
    TRUE ~ NA_real_  # Handle unexpected cases
  ))



combined_sample_report_wide$trial_type <- factor(combined_sample_report_wide$trial_type)

# Perform the repeated measures ANOVA
anova_results <- anova_test(
  data = combined_sample_report_wide, 
  dv = WM_bias, 
  wid = participant, 
  within = trial_type
)

# Print the results
anova_results

```








##### Check that LTM reports were not additionally biased by the (random) initial orientations in which the WM probes first appeared on screen


```{r LTM Bias by random probe orientation, echo=FALSE}

test_2_data2 <- subset(data_LTM, trial_type == "Test 2", select = c("participant","ltm_image","this_ori_ltm","ori_ltm"))
test_2_data_WM2 <- subset(data_WM_long, trial_type_new == "Test 2", select = c("participant","test1","test2","start_distance_t1","start_distance_t2"))
test_2_data2 <- rbind.fill(test_2_data2,test_2_data_WM2[c("participant","test1","test2","start_distance_t1","start_distance_t2")])

test2_trials2 <-split(test_2_data2,test_2_data2$participant)

for(df in 1:length(test2_trials2)) {
  test2_trials2[[df]]$alternative_memory2 <- sapply(test2_trials2[[df]]$ltm_image,function(x) test2_trials2[[df]]$start_distance_t2[grep(x,test2_trials2[[df]]$test2)[1]])
}

test_2_data2 <- rbindlist(test2_trials2)

test_2_data2 <- test_2_data2 %>% 
  rowwise() %>%
  mutate(min_angle_ltm = abs(ori_ltm - alternative_memory2))%>%
  mutate(max_angle_ltm = 360-abs(ori_ltm - alternative_memory2))

test_2_data2 <- transform(test_2_data2, accuracy_ltm = pmin(min_angle_ltm, max_angle_ltm))#angular difference

# extract data for WM bias in baseline over participants

test_baseline_data2 <- subset(data_LTM, trial_type == "Baseline", select = c("participant","ltm_image","this_ori_ltm","ori_ltm"))
data_baseline2 <- subset(data_WM_long, trial_type_new == "Baseline", select = c("participant","test1","test2","start_distance_t1","start_distance_t2"))
test_baseline_data2 <- rbind.fill(test_baseline_data2,data_baseline2[c("participant","test1","test2","start_distance_t1","start_distance_t2")])

baseline_trials2 <-split(test_baseline_data2,test_baseline_data2$participant)

for(df in 1:length(baseline_trials2)) {
  baseline_trials2[[df]]$alternative_memory2 <- sapply(baseline_trials2[[df]]$ltm_image,function(x) baseline_trials2[[df]]$start_distance_t1[grep(x,baseline_trials2[[df]]$test1)[1]])
}

baseline_trials2 <- rbindlist(baseline_trials2)

baseline_trials2 <- baseline_trials2 %>% 
  rowwise() %>%
  mutate(min_angle_ltm = abs(ori_ltm - alternative_memory2))%>%
  mutate(max_angle_ltm = 360-abs(ori_ltm - alternative_memory2))

baseline_trials2 <- transform(baseline_trials2, accuracy_ltm = pmin(min_angle_ltm, max_angle_ltm))#angular difference

# extract data for WM bias in Test 1 over participants

test_1_data2 <- subset(data_LTM, trial_type == "Test 1", select = c("participant","ltm_image","this_ori_ltm","ori_ltm"))
test_1_data_WM2 <- subset(data_WM_long, trial_type_new == "Test 1", select = c("participant","test1","test2","start_distance_t1","start_distance_t2"))
test_1_data2 <- rbind.fill(test_1_data2,test_1_data_WM2[c("participant","test1","test2","start_distance_t1","start_distance_t2")])

test1_trials2 <-split(test_1_data2,test_1_data2$participant)

for(df in 1:length(test1_trials2)) {
  test1_trials2[[df]]$alternative_memory2 <- sapply(test1_trials2[[df]]$ltm_image,function(x) test1_trials2[[df]]$start_distance_t1[grep(x,test1_trials2[[df]]$test1)[1]])
}

test_1_data2 <- rbindlist(test1_trials2)


test_1_data2 <- test_1_data2 %>% 
  rowwise() %>%
  mutate(min_angle_ltm = abs(ori_ltm - alternative_memory2))%>%
  mutate(max_angle_ltm = 360-abs(ori_ltm - alternative_memory2))

test_1_data2 <- transform(test_1_data2, accuracy_ltm = pmin(min_angle_ltm, max_angle_ltm))#angular difference

# add trials type 
test_1_data2$trial_type <- "Test 1"
test_2_data2$trial_type <- "Test 2"
baseline_trials2$trial_type <- "Baseline"


alternative_ltm3 <- rbind(test_1_data2,test_2_data2,baseline_trials2)

summary_stats_participant_random_probe <- summarySE(alternative_ltm3, measurevar = "accuracy_ltm", groupvars = c("participant","trial_type"),na.rm = T)

summary_study_level_random_probe <- 
summarySE(summary_stats_participant_random_probe, measurevar = "accuracy_ltm", groupvars = c("trial_type"),na.rm = T)

# check if the random probe is sig. biasing

t_test_slopes_LTM <- t.test(summary_stats_participant_random_probe$accuracy_ltm, mu = 90)


cohen_d_one_sample <- function(x, mu = 90) {
  d <- (mean(x, na.rm = TRUE) - mu) / sd(x, na.rm = TRUE)
  return(d)
}

# Updated pipeline with effect size
results <- summary_stats_participant_random_probe %>%
  group_by(trial_type) %>%
  nest() %>%
  mutate(
    # Perform the two-sided t-test
    t_test_result = map(data, ~t.test(.x$accuracy_ltm, mu = 90, alternative = "two.sided")),
    
    # Extract the p-values using broom::tidy
    p_value = map_dbl(t_test_result, ~tidy(.x)$p.value),
    
    # Compute Cohen's d
    cohen_d = map_dbl(data, ~cohen_d_one_sample(.x$accuracy_ltm, mu = 90))
  ) %>%
  ungroup() %>%
  
  # Apply Holm-Bonferroni correction
  mutate(
    p_value_adjusted = p.adjust(p_value, method = "holm")
  ) %>%
  select(trial_type, t_test_result, p_value, p_value_adjusted, cohen_d)

# View the results
results

results[[2]][[1]][["p.value"]]
results[[2]][[2]][["p.value"]]
results[[2]][[3]][["p.value"]]

print("The LTM accuracy does not seem to be sig. biased by random WM probe orientations")
t_test_slopes_LTM
```

In response to a reviewer comment, we are adding the statistical test that concern repulsion bias below

```{r intra trial bias results, echo = FALSE}
print(summary_stats)

print("These are the results of the repulsion bias we see in Test 1 and Test 2 in WM")
# T-Test für Test 1
t_test_mean_t1 <- t.test(participant_means$mean_error_to_other_t1, mu = 90)
print("Test 1")
print(t_test_mean_t1)

m_t1 <- mean(participant_means$mean_error_to_other_t1, na.rm = TRUE)
s_t1 <- sd(participant_means$mean_error_to_other_t1, na.rm = TRUE)
cohen_d_t1_manual <- (m_t1 - 90) / s_t1
print(cohen_d_t1_manual)

# T-Test für Test 2
t_test_mean_t2 <- t.test(participant_means$mean_error_to_other_t2, mu = 90)
print("Test 2")
print(t_test_mean_t2)

m_t2 <- mean(participant_means$mean_error_to_other_t2, na.rm = TRUE)
s_t2 <- sd(participant_means$mean_error_to_other_t2, na.rm = TRUE)
cohen_d_t2_manual <- (m_t2 - 90) / s_t2

print(cohen_d_t2_manual)

t_test_t1_vs_t2 <- t.test(participant_means$mean_error_to_other_t1,
                          participant_means$mean_error_to_other_t2,
                          paired = TRUE)

print("Comparison of Test 1 and Test 2 (WM)")
print(t_test_t1_vs_t2)

# Compute the difference between paired samples
differences <- participant_means$mean_error_to_other_t1 - participant_means$mean_error_to_other_t2

# Calculate paired Cohen's d manually
mean_diff <- mean(differences, na.rm = TRUE)
sd_diff <- sd(differences, na.rm = TRUE)
cohens_d_paired_manual <- mean_diff / sd_diff

print(cohens_d_paired_manual)


# Doing the same analysis for the LTM data
wm_long_images <- data_WM_long %>%
  # First image entry
  transmute(
    participant,
    wm_image = image1,
    start_ori = startOri1,
    other_ori = startOri2
  ) %>%
  bind_rows(
    # Second image entry
    data_WM_long %>%
      transmute(
        participant,
        wm_image = image2,
        start_ori = startOri2,
        other_ori = startOri1
      )
  ) %>%
  distinct()

data_LTM_joined <- data_LTM %>%
  left_join(
    wm_long_images %>%
      select(participant, wm_image, start_ori, other_ori),
    by = c("participant", "ltm_image" = "wm_image")
  )


data_LTM_joined_filtered <- data_LTM_joined %>%
  filter(trial_type != "Baseline")

data_LTM_joined_filtered <- data_LTM_joined_filtered %>%
  mutate(
    other_ori = na_if(other_ori, "_"),          # Replace "_" with NA
    other_ori = as.numeric(other_ori),          # Convert to numeric
    ori_ltm = as.numeric(ori_ltm)               # Ensure response is also numeric
  )

# Compute absolute angular error to the other item
data_LTM_joined_filtered$error_to_other_ltm_abs <- apply(
  data_LTM_joined_filtered[, c("ori_ltm", "other_ori")],
  1,
  function(x) abs(angdiff(as.numeric(x[1]), as.numeric(x[2]))$delta_deg)
)

# One-sample t-test against 90°
t.test(data_LTM_joined_filtered$error_to_other_ltm_abs, mu = 90)


participant_means_ltm <- data_LTM_joined_filtered %>%
  group_by(participant) %>%
  summarise(mean_error_to_other_ltm_abs = mean(error_to_other_ltm_abs, na.rm = TRUE))

t_test_ltm_participant_level <- t.test(participant_means_ltm$mean_error_to_other_ltm_abs, mu = 90)






# --- Compute participant-wise means per test type ---
participant_means_ltm_by_test <- data_LTM_joined_filtered %>%
  filter(trial_type %in% c("Test 1", "Test 2")) %>%
  group_by(participant, trial_type) %>%
  summarise(mean_error = mean(error_to_other_ltm_abs, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = trial_type, values_from = mean_error, names_prefix = "mean_error_to_other_")

# --- T-Tests ---
# One-sample t-tests against 90°

# Clean up column names to remove spaces
colnames(participant_means_ltm_by_test) <- make.names(colnames(participant_means_ltm_by_test))

# Check column names again
t_test_ltm_t1 <- t.test(participant_means_ltm_by_test$mean_error_to_other_Test.1, mu = 90)
m <- mean(participant_means_ltm_by_test$mean_error_to_other_Test.1, na.rm = TRUE)
s <- sd(participant_means_ltm_by_test$mean_error_to_other_Test.1, na.rm = TRUE)
cohen_d_manual_t1 <- (m - 90) / s

# SE for Test 1
m_t1 <- mean(participant_means_ltm_by_test$mean_error_to_other_Test.1, na.rm = TRUE)
s_t1 <- sd(participant_means_ltm_by_test$mean_error_to_other_Test.1, na.rm = TRUE)
se_t1 <- s_t1 / sqrt(sum(!is.na(participant_means_ltm_by_test$mean_error_to_other_Test.1)))


t_test_ltm_t2 <- t.test(participant_means_ltm_by_test$mean_error_to_other_Test.2, mu = 90)

m <- mean(participant_means_ltm_by_test$mean_error_to_other_Test.2, na.rm = TRUE)
s <- sd(participant_means_ltm_by_test$mean_error_to_other_Test.2, na.rm = TRUE)
cohen_d_manual_t2 <- (m - 90) / s

m_t2 <- mean(participant_means_ltm_by_test$mean_error_to_other_Test.2, na.rm = TRUE)
s_t2 <- sd(participant_means_ltm_by_test$mean_error_to_other_Test.2, na.rm = TRUE)
se_t2 <- s_t2 / sqrt(sum(!is.na(participant_means_ltm_by_test$mean_error_to_other_Test.2)))



t_test_ltm_t1_vs_t2 <- t.test(
  participant_means_ltm_by_test$mean_error_to_other_Test.1,
  participant_means_ltm_by_test$mean_error_to_other_Test.2,
  paired = TRUE
)

diff_ltm <- participant_means_ltm_by_test$mean_error_to_other_Test.1 -
            participant_means_ltm_by_test$mean_error_to_other_Test.2

# Compute Cohen's d for paired samples
mean_diff_ltm <- mean(diff_ltm, na.rm = TRUE)
sd_diff_ltm <- sd(diff_ltm, na.rm = TRUE)
cohens_d_ltm_manual <- mean_diff_ltm / sd_diff_ltm



# --- Print results ---
print("These are the results of the repulsion bias we see in Test 1 and Test 2 in LTM")
print("Test 1")
print(t_test_ltm_t1)
cohen_d_manual_t1
se_t1
print("Test 2")
print(t_test_ltm_t2)
cohen_d_manual_t2
se_t2

print("Comparison of Test 1 and Test 2")
print(t_test_ltm_t1_vs_t2)
print(cohens_d_ltm_manual)


```
